import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
from keras.layers.core import Dense, Activation, Dropout
from keras.layers import LSTM
from keras.models import Sequential, load_model
from keras.callbacks import CSVLogger, ReduceLROnPlateau
from keras.optimizers import adam_v2
import transbigdata as tbd # 轨迹处理包，此处用于显示底图
import warnings

warnings.filterwarnings("ignore")
#设置随机种子，让每次结果都一样，方便对照
np.random.seed(120)
tf.random.set_seed(120)
# 支持中文
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号


# 读入轨迹序列数据
data = pd.read_csv('./train.csv',index_col=0).iloc[:, 0:3]
data.head()


# 经纬度绘制范围
bound = [min(data['lon'])-0.005, min(data['lat'])-0.005,
         max(data['lon'])+0.005, max(data['lat'])+0.005]
fig = plt.figure(figsize=(20, 15))
# 绘制武汉市底图
tbd.plot_map(plt,bound, zoom=12, style=0)
for tri in set(data['tri_id']):
    temp = data.loc[data.tri_id == tri]
    # 绘制单个轨迹
    plt.plot(temp['lon'],temp['lat'],'-b')
plt.show()


def create_dataset(data, train_num,max_min):
    """
    :param data:  		轨迹数据集合
    :param train_num: 	多少个数据一组
    :param max_min:		用来归一化
    :return: 	  		数据、标签、用来反归一化的最大最小值
    """
    train_seq = []
    train_label = []
    m,n = max_min
    # 对轨迹一条一条的进行处理
    for tri_id in set(data['tri_id']):
        data_temp = data.loc[data.tri_id == tri_id]
        # 得到经度、纬度
        data_temp = np.array(data_temp.iloc[:,1:3])
        # 标准化
        data_temp = (data_temp - n) / (m - n)

        for i in range(data_temp.shape[0] - train_num):
            x = []
            for j in range(i, i + train_num):
                x.append(list(data_temp[j,:]))
            train_seq.append(x)
            train_label.append(data_temp[i + train_num,:])

    train_seq = np.array(train_seq, dtype='float64')
    train_label = np.array(train_label, dtype='float64')

    return train_seq,train_label

#4
def trainModel(train_X, train_Y):
    model = Sequential()
    model.add(LSTM(120,input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(120,return_sequences=False))
    model.add(Dropout(0.3))
    model.add(Dense(train_Y.shape[1]))
    model.add(Activation("relu"))
    adam = adam_v2.Adam(learning_rate=0.001)

    model.compile(loss='mse', optimizer=adam, metrics=['acc'])
    # 训练日志
    log = CSVLogger("./csvlog_lon_lat_one_step.csv",separator=",",append=True)
    reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=1, verbose=1,
                               mode='auto', epsilon=0.0001, cooldown=0, min_lr=0.0000001)
    model.fit(train_X, train_Y, epochs=20, batch_size=64, verbose=1,validation_split=0.1,callbacks=[log,reduce])
    # 打印神经网络结构，统计参数数目
    model.summary()

    return model


#5
# 20 个为一组
train_num = 20
# 得到归一化参数
nor = np.array(data.loc[:, ['lon', 'lat']])
m = nor.max(axis=0)
n = nor.min(axis=0)
max_min = [m, n]
#生成训练数据
train_seq,train_label = create_dataset(data, train_num, max_min)
print("data ", data.shape)
print("train_seq ", train_seq.shape)
print("train_label ", train_label.shape)

# 训练模型
model = trainModel(train_seq, train_label)
loss, acc = model.evaluate(train_seq, train_label, verbose=1)
print('Loss : {}, Accuracy: {}'.format(loss, acc * 100))

# 保存模型
model.save("./traj_mode_wuhan_lon_lat_one_step_20.h5")


#6
logs = pd.read_csv("./csvlog_lon_lat_one_step.csv")

fig, ax = plt.subplots(2,2,figsize=(8,8))
ax[0][0].plot(logs['epoch'],logs['acc'], label='acc')
ax[0][0].set_title('acc')

ax[0][1].plot(logs['epoch'],logs['loss'], label='loss')
ax[0][1].set_title('loss')

ax[1][0].plot(logs['epoch'],logs['val_acc'], label='val_acc')
ax[1][0].set_title('val_acc')

ax[1][1].plot(logs['epoch'],logs['val_loss'], label='val_loss')
ax[1][1].set_title('val_loss')

plt.show()


# 7
data_all = pd.read_csv('./test.csv',index_col=0).iloc[:, 0:3]
li = list(set(data_all['tri_id']))
test_data = data_all.loc[data_all.tri_id == li[0]]
test_data.head()


#
